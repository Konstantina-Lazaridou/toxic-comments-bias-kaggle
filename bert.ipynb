{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61fffd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "939b53e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU used? (0=yes, -1=no): 0\n"
     ]
    }
   ],
   "source": [
    "current_cuda_device = -1\n",
    "if torch.cuda.is_available():\n",
    "    current_cuda_device = torch.cuda.current_device()\n",
    "print(f'Is GPU used? (0=yes, -1=no): {current_cuda_device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45eb4a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "CRAWL_EMBEDDING_PATH = '/home/konstantina/data/toxic-comments-bias-kaggle/crawl-300d-2M.vec'\n",
    "GLOVE_EMBEDDING_PATH = '/home/konstantina/data/toxic-comments-bias-kaggle/glove.840B.300d.txt'\n",
    "NUM_MODELS = 2\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c47dad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    '''\n",
    "    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n",
    "    '''\n",
    "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "    def clean_special_chars(text, punct):\n",
    "        for p in punct:\n",
    "            text = text.replace(p, ' ')\n",
    "        return text\n",
    "\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56fb4cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test shapes: (1804874, 45), (97320, 2)\n",
      "Test private and test public shapes: (97320, 45), (97320, 45)\n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "train = pd.read_csv('/home/konstantina/data/toxic-comments-bias-kaggle/train.csv')\n",
    "test = pd.read_csv('/home/konstantina/data/toxic-comments-bias-kaggle/test.csv')\n",
    "test_private = pd.read_csv('/home/konstantina/data/toxic-comments-bias-kaggle/test_private_expanded.csv')\n",
    "test_public = pd.read_csv('/home/konstantina/data/toxic-comments-bias-kaggle/test_public_expanded.csv')\n",
    "# id,target,comment_text,severe_toxicity,obscene,identity_attack,insult,threat,asian,atheist,bisexual,black,buddhist,christian,female,heterosexual,hindu,homosexual_gay_or_lesbian,intellectual_or_learning_disability,jewish,latino,male,muslim,other_disability,other_gender,other_race_or_ethnicity,other_religion,other_sexual_orientation,physical_disability,psychiatric_or_mental_illness,transgender,white,created_date,publication_id,parent_id,article_id,rating,funny,wow,sad,likes,disagree,sexual_explicit,identity_annotator_count,toxicity_annotator_count\n",
    "print(f'Train and test shapes: {train.shape}, {test.shape}')\n",
    "print(f'Test private and test public shapes: {test_private.shape}, {test_public.shape}')  # all features and binarized toxicity\n",
    "\n",
    "# preprocess\n",
    "x_train = preprocess(train['comment_text'])\n",
    "x_test = preprocess(test['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2e5dc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train: [0 0 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# targets\n",
    "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
    "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "print(f'y_train: {y_train}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04ad96d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and vectorize text\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))  # fit both vocabularies\n",
    "x_train = tokenizer.texts_to_sequences(x_train)  # translate into integers\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)  # pad for balanced text length\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "287b403e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: 327009\n"
     ]
    }
   ],
   "source": [
    "vocabulary = None\n",
    "vocabulary = vocabulary or len(tokenizer.word_index) + 1\n",
    "print(f'vocabulary: {vocabulary}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "385b96ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    unknown_words = []\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b9658c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31adc543d02c4c73b5b76b6a7a46b1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\n",
    "print('n unknown words (crawl): ', len(unknown_words_crawl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e7a3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
